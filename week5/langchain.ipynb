{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b41f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saud/Desktop/LLM_Projects/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39dbdeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e945c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "vector_db = 'vector_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794069d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "folders = glob.glob('knowledge-base/*')\n",
    "for folder in folders:\n",
    "    doc_type = folder.split(os.sep)[-1]\n",
    "    loader = DirectoryLoader(folder, glob= \"**/*.md\", loader_cls= TextLoader)\n",
    "    folder_docs = loader.load()\n",
    "  \n",
    "    for doc in folder_docs:\n",
    "        doc.metadata['doc_type'] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da97132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 1088, chunk_overlap = 200)\n",
    "spllited_text = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a990b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: company, employees, products, contracts\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in spllited_text)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af8c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(vector_db):\n",
    "#     Chroma(persist_directory= vector_db, embedding_function= embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67cf4ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorStore = Chroma.from_documents(embedding= embeddings, persist_directory= vector_db, documents= spllited_text)\n",
    "# print(f\"Vector Store created with {vectorStore._collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aafcfcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection = vectorStore._collection\n",
    "# result = collection.get(limit=1, include=['embeddings'])['embeddings'][0]\n",
    "# dimensions =  len(result)\n",
    "# print(f\"The vector has {dimensions} dimensions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a68e531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store created with 1536 dimensions and 109 documents\n"
     ]
    }
   ],
   "source": [
    "vectorStore = FAISS.from_documents(embedding= embeddings, documents= spllited_text)\n",
    "print(f\"Vector Store created with {vectorStore.index.d} dimensions and {vectorStore.index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8666408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_pipeline(prompt, history):\n",
    "    llm = ChatOpenAI(temperature= 0.5, model= MODEL)\n",
    "    retriever = vectorStore.as_retriever(search_kwargs = {\"k\": 20})\n",
    "    memory = ConversationBufferMemory(memory_key= 'chat_history', return_messages= True)\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "\n",
    "    return conversation_chain.invoke({'question': prompt, \"chat_history\": history})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0880e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(RAG_pipeline).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a97baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_testing_pipeline(prompt, history):\n",
    "    llm = ChatOpenAI(temperature= 0.5, model= MODEL)\n",
    "    retriever = vectorStore.as_retriever()\n",
    "    memory = ConversationBufferMemory(memory_key= 'chat_history', return_messages= True)\n",
    "\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks= [StdOutCallbackHandler()])\n",
    "\n",
    "    return conversation_chain.invoke({'question': prompt, \"chat_history\": history})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0f1e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/3c4w5v3147lfbftqk954bsxh0000gn/T/ipykernel_75915/2781222458.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key= 'chat_history', return_messages= True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "## Annual Performance History\n",
      "- **2019:** Exceeds Expectations - Continuously delivered high-quality code and participated actively in team meetings.\n",
      "- **2020:** Meets Expectations - Jordan K. Bishop maintained steady performance but faced challenges due to a higher workload from multiple projects.\n",
      "- **2021:** Exceeds Expectations - Recognized for leadership during the customer portal project; received the “Innovation Award” for creative problem-solving.\n",
      "- **2022:** Meets Expectations - While mentoring others, the shift in focus led to fewer contributions to new features, marking a decrease in performance.\n",
      "- **2023:** Needs Improvement - Transitioning back to development has resulted in difficulties with recent technologies, prompting a performance improvement plan.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2023:** Rating: 4.5/5  \n",
      "  *Samuel exceeded expectations, successfully leading a cross-departmental project on AI-driven underwriting processes.*\n",
      "\n",
      "- **2022:** Rating: 3.0/5  \n",
      "  *Some challenges in meeting deadlines and collaboration with the engineering team. Received constructive feedback and participated in a team communication workshop.*\n",
      "\n",
      "- **2021:** Rating: 4.0/5  \n",
      "  *There was notable improvement in performance. Worked to enhance model accuracy, leading to improved risk assessment outcomes for B2C customers.*\n",
      "\n",
      "- **2020:** Rating: 3.5/5  \n",
      "  *Exhibited a solid performance during the initial year as a Senior Data Scientist but had struggles adapting to new leadership expectations.*\n",
      "\n",
      "## Compensation History\n",
      "- **2023:** Base Salary: $115,000 + Bonus: $15,000  \n",
      "  *Annual bonus based on successful project completions and performance metrics.*\n",
      "\n",
      "- **2022:** Base Salary: $110,000 + Bonus: $10,000  \n",
      "  *Slight decrease in bonus due to performance challenges during the year.*\n",
      "\n",
      "---\n",
      "\n",
      "## Annual Performance History\n",
      "- **2023**:  \n",
      "  - Performance Rating: Exceeds Expectations  \n",
      "  - Key Achievements: Led the \"Tech the Halls\" campaign that resulted in a 50% increase in leads during the holiday season. \n",
      "  - Emily Tran's innovative strategies and attention to detail have made her stand out among her peers.\n",
      "\n",
      "- **2022**:  \n",
      "  - Performance Rating: Meets Expectations  \n",
      "  - Key Achievements: Enhanced Insurellm's email marketing strategy, achieving a 25% open rate increase.\n",
      "\n",
      "- **2021**:  \n",
      "  - Performance Rating: Meets Expectations  \n",
      "  - Key Achievements: Contributed to the launch of a customer referral program that resulted in a 15% growth in B2C customers.\n",
      "\n",
      "---\n",
      "\n",
      "## Compensation History\n",
      "- **2023**:  \n",
      "  - Base Salary: $75,000  \n",
      "  - Bonus: $10,000 for exceeding annual targets.\n",
      "\n",
      "- **2022**:  \n",
      "  - Base Salary: $70,000  \n",
      "  - Bonus: $5,000 for achieving marketing milestones.\n",
      "\n",
      "- **2021**:  \n",
      "  - Base Salary: $67,500  \n",
      "  - No bonus due to reallocation of marketing funds during the pandemic.\n",
      "\n",
      "---\n",
      "\n",
      "## Other HR Notes\n",
      "- Oliver enjoys a strong rapport with team members and is known for organizing regular team-building activities.\n",
      "- Participated in Insurellm’s Hackathon in 2022, where he led a project that won “Best Overall Solution.” \n",
      "- Pursuing AWS Certified Solutions Architect certification to enhance cloud skillset.\n",
      "- Has expressed interest in further leadership opportunities within Insurellm and may consider project management roles in the future.\n",
      "Human: Who won the prestigious IIOTY award in 2023?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_testing_pipeline('Who won the prestigious IIOTY award in 2023?', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e461e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
